import wandb 
from tqdm import tqdm
import os

import torch
import torch.nn
import omegaconf
from omegaconf import OmegaConf
from transformers import set_seed
from transformers import get_constant_schedule
from transformers import get_linear_schedule_with_warmup
from torch.optim import AdamW 

from utils import load_config # , set_logger
from model import EncoderForClassification
from data import IMDBDataset, get_dataloader

# torch.cuda.set_per_process_memory_fraction(11/24) -> ê¹€ì¬í¬ ë¡œì»¬ê³¼ ì‹ ì…ìƒ ë¡œì»¬ì˜ vram ë§ì¶”ê¸° ìš©ë„. ê³¼ì œ ìˆ˜í–‰ ì‹œ ì‚­ì œí•˜ì…”ë„ ë©ë‹ˆë‹¤. 
# modelê³¼ dataì—ì„œ ì •ì˜ëœ custom class ë° functionì„ importí•©ë‹ˆë‹¤.
"""
ì—¬ê¸°ì„œ import í•˜ì‹œë©´ ë©ë‹ˆë‹¤. 
"""

# ======== ì‹œë“œ ì„¤ì • ë¶€ë¶„ ========
set_seed(42)
# ==============================

def train_iter(model, inputs, optimizer, device):
    inputs = {key : value.to(device) for key, value in inputs.items()}
    # ëª¨ë¸ì´ (logits, loss) íŠœí”Œì„ ë°˜í™˜í•˜ë¯€ë¡œ, ê°ê°ì˜ ë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤.
    logits, loss = model(**inputs)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    wandb.log({'train_loss' : loss.item()})
    return loss

def valid_iter(model, inputs, device):
    model.eval()
    with torch.no_grad():
        inputs = {key : value.to(device) for key, value in inputs.items()}
        # forward args ë§Œë“¤ê¸°
        forward_args = {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "labels": inputs["labels"]
        }
        if "token_type_ids" in inputs:
            forward_args["token_type_ids"] = inputs["token_type_ids"]

        outputs_logits, outputs_loss = model(**forward_args)
        accuracy = calculate_accuracy(outputs_logits, inputs['labels'])
    return outputs_loss.item(), accuracy

def calculate_accuracy(logits, labels):
    preds = logits.argmax(dim=-1)
    correct = (preds == labels).sum().item()
    return correct / labels.size(0)

def main(configs : omegaconf.DictConfig) :

    if configs.log_config.use_wandb:
        wandb.init(
            project="DSBA_NLP-classification-exp",    # ğŸ‘ˆ wandb í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì§€ì •í•˜ì„¸ìš”.
            # name=f"exp-{wandb.util.generate_id()}", # ğŸ‘ˆ ì‹¤í—˜ì˜ ê³ ìœ í•œ ì´ë¦„ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            name=f"NLP-exp-roberta-base_with-linear-scheduler", # ğŸ‘ˆ ì‹¤í—˜ì˜ ê³ ìœ í•œ ì´ë¦„ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            config=OmegaConf.to_container(configs, resolve=True) # ì„¤ì •ê°’ì„ wandbì— ì €ì¥gi
        )
    # =======================================================

    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # torch.cuda.set_per_process_memory_fraction(11/24) # í•„ìš”ì‹œ ì‚¬ìš© (ì£¼ì„ í•´ì œ)

    # Load model
    print("Loading model...")
    model = EncoderForClassification(model_config=configs.model_config).to(device)
    print("Model loaded.")

    # Load data
    print("Loading data...")
    train_dataloader = get_dataloader(data_config=configs.data_config, split='train')
    valid_dataloader = get_dataloader(data_config=configs.data_config, split='valid')
    test_dataloader = get_dataloader(data_config=configs.data_config, split='test') # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”ë„ ë¡œë“œ
    print("Data loaded.")

    # Set optimizer
    # AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© (íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ê¶Œì¥)
    optimizer = AdamW(model.parameters(), lr=configs.optimizer_config.learning_rate)
    
    # í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    # total_steps = len(train_dataloader) * configs.train_config.epochs
    # warmup_steps = int(total_steps * configs.optimizer_config.warmup_ratio)
    # scheduler = get_constant_schedule_with_warmup(
    #     optimizer, num_warmup_steps=warmup_steps
    # )
    '''
    scheduler = get_constant_schedule(optimizer)
    '''
    # ì„ í˜• ìŠ¤ì¼€ì¥´ëŸ¬
    total_steps = len(train_dataloader) * configs.train_config.epochs
    warmup_steps = int(total_steps * 0.1)  # 10% warmup

    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps
    )

    # print(f"Optimizer and scheduler set. Total steps: {total_steps}, Warmup steps: {warmup_steps}")
    print(f"Optimizer and scheduler set. Learning rate will be constant after warmup (if any).")

    # Train & validation for each epoch
    print("Starting training...")
    best_valid_accuracy = 0.0 # ìµœê³  ê²€ì¦ ì •í™•ë„ë¥¼ ì €ì¥
    for epoch in range(configs.train_config.epochs) :
        # Train Loop
        train_losses = []
        # tqdmìœ¼ë¡œ í•™ìŠµ ì§„í–‰ë¥  í‘œì‹œ
        train_pbar = tqdm(train_dataloader, desc=f"Epoch {epoch+1} Train")
        for batch in train_pbar:
            loss = train_iter(model, batch, optimizer, device)
            train_losses.append(loss)
            scheduler.step() # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            train_pbar.set_postfix({'loss': loss}) # tqdmì— í˜„ì¬ ì†ì‹¤ í‘œì‹œ
        
        avg_train_loss = sum(train_losses) / len(train_losses)
        print(f"Epoch {epoch+1} Train Avg Loss: {avg_train_loss:.4f}")
        wandb.log({"avg_train_loss": avg_train_loss, "epoch": epoch})


        # Validation Loop
        valid_losses = []
        valid_accuracies = []

        valid_pbar = tqdm(valid_dataloader, desc=f"Epoch {epoch+1} Valid")
        for step, batch in enumerate(valid_pbar):
            loss, accuracy = valid_iter(model, batch, device)
            valid_losses.append(loss)
            valid_accuracies.append(accuracy)

            # tqdm í‘œì‹œ
            valid_pbar.set_postfix({'loss': loss, 'accuracy': accuracy})

            # ğŸ”¥ ë§¤ ìŠ¤í…ë§ˆë‹¤ wandb ë¡œê¹…
            wandb.log({
                "val_loss_step": loss,
                "val_accuracy_step": accuracy,
                "epoch": epoch,
                "val_step": step
            })

        # Epoch ë‹¨ìœ„ í‰ê·  ë¡œê¹…
        avg_valid_loss = sum(valid_losses) / len(valid_losses)
        avg_valid_accuracy = sum(valid_accuracies) / len(valid_accuracies)
        print(f"Epoch {epoch+1} Valid Avg Loss: {avg_valid_loss:.4f}, Avg Accuracy: {avg_valid_accuracy:.4f}")

        wandb.log({
            "avg_valid_loss": avg_valid_loss,
            "avg_valid_accuracy": avg_valid_accuracy,
            "epoch": epoch
        })

        # ìµœê³  ì •í™•ë„ ëª¨ë¸ ì €ì¥ (ì„ íƒ ì‚¬í•­)
        if avg_valid_accuracy > best_valid_accuracy:
            best_valid_accuracy = avg_valid_accuracy
            # model.state_dict()ëŠ” ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë“¤ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            torch.save(model.state_dict(), os.path.join(configs.output_dir, "best_model.pt"))
            print(f"Best validation accuracy achieved! Saving model with accuracy: {best_valid_accuracy:.4f}")

    print("Training finished.")
    
    # validation for last epoch
    print("\nStarting final test with the best model...")
    # ìµœì  ëª¨ë¸ ë¡œë“œ (ì €ì¥í–ˆë‹¤ë©´)
    if os.path.exists(os.path.join(configs.output_dir, "best_model.pt")):
        model.load_state_dict(torch.load(os.path.join(configs.output_dir, "best_model.pt")))
        print("Loaded best model for final testing.")
    
    test_losses = []
    test_accuracies = []
    test_pbar = tqdm(test_dataloader, desc="Final Test")
    for batch in test_pbar:
        loss, accuracy = valid_iter(model, batch, device) # valid_iterë¥¼ ì¬í™œìš©
        test_losses.append(loss)
        test_accuracies.append(accuracy)
        test_pbar.set_postfix({'loss': loss, 'accuracy': accuracy})

    avg_test_loss = sum(test_losses) / len(test_losses)
    avg_test_accuracy = sum(test_accuracies) / len(test_accuracies)
    print(f"Final Test Avg Loss: {avg_test_loss:.4f}, Avg Accuracy: {avg_test_accuracy:.4f}")
    if configs.log_config.use_wandb:
        wandb.log({"final_test_loss": avg_test_loss, "final_test_accuracy": avg_test_accuracy})
        wandb.finish() # wandb ì„¸ì…˜ ì¢…ë£Œ

    
    
if __name__ == "__main__" :
    configs = load_config(config_path="../configs/config_roberta.yaml")
    os.makedirs(configs.output_dir, exist_ok=True) # output_dirì´ ì—†ìœ¼ë©´ ìƒì„±
    main(configs)